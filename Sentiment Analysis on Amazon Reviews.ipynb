{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILQLfX94zAJD"
   },
   "source": [
    "# Sentiment Analysis on Amazon Reviews\n",
    "**Author:** Meri Asatryan  \n",
    "**Date:** April 17, 2025  \n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, we perform sentiment analysis on the [Amazon Review Polarity Dataset](https://www.kaggle.com/datasets/bittlingmayer/amazonreviews). The goal is to classify product reviews as either **positive** or **negative** based on their content.\n",
    "\n",
    "We explore both **classical machine learning models** (Naive Bayes, Logistic Regression, and SVM) and a **transformer-based deep learning model** (BERT) to compare performance and demonstrate the benefits of modern NLP techniques.\n",
    "\n",
    "---\n",
    "\n",
    "### Key Components:\n",
    "- **Data Cleaning and Preprocessing** using NLTK and regex\n",
    "- **Exploratory Data Analysis** with word clouds and visualizations\n",
    "- **Classical ML Models** with TF-IDF features\n",
    "- **Fine-tuned BERT Model** using HuggingFace Transformers\n",
    "- **Model Evaluation** with precision, recall, F1-score, and accuracy\n",
    "- **Real-time Sentiment Prediction** using the trained model\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dskFXRuzpsR"
   },
   "source": [
    "### Setting Up the Environment\n",
    "\n",
    "We begin by **installing** all the necessary Python packages and **importing** modules that we‚Äôll use throughout the notebook. This includes libraries for:\n",
    "- Text processing (`nltk`, `re`, `tqdm`)\n",
    "- Visualization (`matplotlib`, `wordcloud`)\n",
    "- Machine learning (`scikit-learn`)\n",
    "- Deep learning (`transformers`, `datasets`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install all required libraries\n",
    "!pip install -U gdown nltk tqdm wordcloud transformers datasets\n",
    "\n",
    "#Imports for data handling and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#Classical ML\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#Transformers\n",
    "from transformers import (\n",
    "    BertTokenizer, BertForSequenceClassification,\n",
    "    Trainer, TrainingArguments, pipeline\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "#Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Setup device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üöÄ Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kybKYrMVzwNl"
   },
   "source": [
    "### Downloading the Dataset\n",
    "\n",
    "We use `gdown` to download the Amazon Review Polarity dataset from a shared Google Drive folder. This dataset contains 3.6 millions of reviews labeled as either positive or negative. It forms the foundation for training and evaluating our sentiment analysis models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --folder https://drive.google.com/drive/folders/1P_T7loBWVaDDl8hRu0W6F5bmwOisQPEv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eelJnWAz2n3"
   },
   "source": [
    "### Loading the Review Data\n",
    "\n",
    "We load the dataset into Pandas DataFrames and assign meaningful column names (`rating`, `review_title`, `review_text`). This step helps us quickly inspect the structure of the data and prepares it for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_file_path = '/content/amazon_review_polarity_csv/train.csv'\n",
    "test_file_path = '/content/amazon_review_polarity_csv/test.csv'\n",
    "\n",
    "\n",
    "train_data = pd.read_csv(train_file_path, header=None)\n",
    "train_data.columns = ['rating', 'review_title', 'review_text']\n",
    "test_data = pd.read_csv(test_file_path)\n",
    "test_data.columns = ['rating', 'review_title', 'review_text']\n",
    "\n",
    "\n",
    "print(train_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.columns)\n",
    "print(test_data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nUs55KWXz9i5"
   },
   "source": [
    "### Cleaning and Normalizing Review Text\n",
    "\n",
    "To make the data usable for machine learning, we clean the review text:\n",
    "- Remove HTML tags, special characters, and numbers\n",
    "- Convert all text to lowercase\n",
    "- Remove common stopwords using NLTK\n",
    "\n",
    "We also use `tqdm` to visually track progress, as this cleaning process is applied to hundreds of thousands of reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Define function to clean text\n",
    "def clean_text(text):\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters, numbers, and extra spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "# Add tqdm to show progress while applying the cleaning function\n",
    "tqdm.pandas()\n",
    "\n",
    "# Clean the review text in both train and test data with progress bar\n",
    "train_data['cleaned_review'] = train_data['review_text'].progress_apply(clean_text)\n",
    "test_data['cleaned_review'] = test_data['review_text'].progress_apply(clean_text)\n",
    "\n",
    "# Display cleaned text\n",
    "print(train_data[['review_text', 'cleaned_review', \"rating\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eylsuybw0Fby"
   },
   "source": [
    "### Inspecting Cleaned Reviews\n",
    "\n",
    "We display examples of the original and cleaned review text side-by-side. This helps confirm that our preprocessing function is working as expected and producing usable input for modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the rating is 1 (negative reviews)\n",
    "negative_reviews = train_data[train_data['rating'] == 2]\n",
    "\n",
    "# Display the relevant columns (review text, cleaned review, and rating)\n",
    "print(negative_reviews[['review_text', 'cleaned_review', 'rating']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-Db1YEY0Kzo"
   },
   "source": [
    "Visualizing Sentiment Distribution (Training Set)\n",
    "\n",
    "To understand how balanced our dataset is, we first count how many reviews are labeled as positive and how many as negative.\n",
    "\n",
    "We then use a bar chart to plot these counts:\n",
    "- **Red bar** for negative sentiment (label 1)\n",
    "- **Green bar** for positive sentiment (label 2)\n",
    "\n",
    "This visualization is crucial because class imbalance can significantly impact model performance. Ideally, we want both classes to be relatively balanced, or we need to account for the imbalance during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of positive and negative reviews\n",
    "sentiment_counts = train_data['rating'].value_counts()\n",
    "\n",
    "# Plot the sentiment distribution\n",
    "plt.figure(figsize=(6, 4))\n",
    "sentiment_counts.plot(kind='bar', color=['red', 'green'])\n",
    "plt.title('Sentiment Distribution (Training Data)')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(ticks=[0, 1], labels=['Negative', 'Positive'], rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6bCVGaf1Gnm"
   },
   "source": [
    "The bar chart shows the number of **positive** and **negative** reviews in the training dataset.\n",
    "\n",
    "**Observation:**\n",
    "- The two bars (red for negative, green for positive) are nearly **equal in height**, meaning the dataset is **perfectly balanced**.\n",
    "- This is ideal for a classification task because the model gets an equal opportunity to learn from both classes.\n",
    "- There's no need for resampling techniques like oversampling or undersampling.\n",
    "\n",
    "**Why it's important:**\n",
    "A balanced dataset ensures that the model:\n",
    "- Doesn‚Äôt become biased toward the majority class\n",
    "- Achieves **more reliable precision and recall** for both positive and negative predictions\n",
    "\n",
    "This strong class balance gives us a good foundation for fair model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lH7tpmfn0Vpu"
   },
   "source": [
    "### Distribution of Review Lengths\n",
    "\n",
    "In this step, we calculate the number of words in each cleaned review. This helps us:\n",
    "- Get a feel for how verbose users are in their reviews\n",
    "- Decide how long input sequences should be when fed into models like BERT\n",
    "- Identify and filter out extremely short or long reviews if necessary\n",
    "\n",
    "We use a histogram to visualize the distribution. Most reviews tend to be between 20‚Äì100 words, which guides padding and truncation settings for tokenization later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the length of each review\n",
    "train_data['review_length'] = train_data['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Plot the distribution of review lengths\n",
    "plt.figure(figsize=(6, 4))\n",
    "train_data['review_length'].plot(kind='hist', bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Distribution of Review Lengths (Training Data)')\n",
    "plt.xlabel('Review Length (words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgVYs3NX1vXy"
   },
   "source": [
    "This histogram visualizes how long the reviews are in terms of **number of words** after text cleaning.\n",
    "\n",
    "---\n",
    "\n",
    "**Key Observations:**\n",
    "- Most reviews fall within the **10 to 60 word** range.\n",
    "- There is a **sharp peak around 15‚Äì20 words**, showing that many users write brief reviews.\n",
    "- The distribution is **right-skewed** (long tail to the right), meaning a smaller number of reviews are significantly longer, going up to **150+ words**.\n",
    "- Very few reviews are longer than 100 words, which helps us decide a good **maximum sequence length** (e.g., 128 tokens) for BERT.\n",
    "\n",
    "---\n",
    "\n",
    "**Why this matters:**\n",
    "- Helps in deciding the **input size** for models like BERT (to avoid unnecessary padding or truncation).\n",
    "- Provides a better understanding of **user writing behavior** ‚Äî most reviews are short, informal, and concise.\n",
    "- We might choose to filter out extremely short reviews (e.g., less than 5 words) if they don‚Äôt provide useful sentiment cues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yRct20wZ3MXX"
   },
   "source": [
    "### Word Cloud of Frequent Bigrams\n",
    "Before generating the word clouds, we enhance the cleaned reviews by removing both general and domain-specific stopwords (like ‚Äúbook‚Äù, ‚Äúmovie‚Äù, ‚Äúreally‚Äù), and convert the text into bigrams‚Äîtwo-word combinations that better capture context.\n",
    "\n",
    "We sample 10,000 reviews from each sentiment class, transform them into bigrams, and combine them into strings. Then, we generate separate word clouds for positive and negative reviews, which help us visualize the most common meaningful phrases used in each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Add more domain-specific stopwords\n",
    "custom_stopwords = set(ENGLISH_STOP_WORDS).union({\"book\", \"one\", \"movie\", \"good\", \"really\", \"time\", \"like\", \"even\", \"use\", \"make\", \"thing\", \"read\", \"music\", \"album\"})\n",
    "\n",
    "# Create bigrams from the cleaned reviews\n",
    "def generate_bigrams(text):\n",
    "    words = text.split()\n",
    "    bigrams = [' '.join([words[i], words[i+1]]) for i in range(len(words)-1)]\n",
    "    return ' '.join(bigrams)\n",
    "\n",
    "positive_reviews = train_data[train_data['rating'] == 2]['cleaned_review'].progress_apply(lambda x: x)\n",
    "negative_reviews = train_data[train_data['rating'] == 1]['cleaned_review'].progress_apply(lambda x: x)\n",
    "\n",
    "positive_reviews_sample = positive_reviews.sample(10000)\n",
    "negative_reviews_sample = negative_reviews.sample(10000)\n",
    "# Apply bigram transformation\n",
    "positive_reviews_bigrams = positive_reviews_sample.apply(generate_bigrams)\n",
    "negative_reviews_bigrams = negative_reviews_sample.apply(generate_bigrams)\n",
    "\n",
    "# Combine the positive and negative reviews into one string\n",
    "positive_text = ' '.join(positive_reviews_bigrams)\n",
    "negative_text = ' '.join(negative_reviews_bigrams)\n",
    "\n",
    "# Generate word clouds with bigrams and refined stopwords\n",
    "positive_wc = WordCloud(width=800, height=400, background_color='white', stopwords=custom_stopwords, max_words=200).generate(positive_text)\n",
    "negative_wc = WordCloud(width=800, height=400, background_color='black', stopwords=custom_stopwords, max_words=200).generate(negative_text)\n",
    "\n",
    "# Plot the word clouds\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(positive_wc, interpolation='bilinear')\n",
    "plt.title('Positive Reviews Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(negative_wc, interpolation='bilinear')\n",
    "plt.title('Negative Reviews Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECTOOQkK3hEK"
   },
   "source": [
    "### Fine-Tuning BERT for Sentiment Classification\n",
    "We use the HuggingFace transformers library to fine-tune a pre-trained BERT model (bert-base-uncased) for binary sentiment classification.\n",
    "\n",
    "Steps include:\n",
    "\n",
    "Installing required libraries and checking GPU availability\n",
    "\n",
    "Converting review ratings into binary labels (0 = negative, 1 = positive)\n",
    "\n",
    "Sampling smaller subsets of the dataset for faster training\n",
    "\n",
    "Tokenizing reviews with padding and truncation\n",
    "\n",
    "Formatting the data into PyTorch-compatible tensors\n",
    "\n",
    "Initializing a BERT model for sequence classification\n",
    "\n",
    "Defining training parameters using TrainingArguments\n",
    "\n",
    "Training the model using the Trainer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# ‚úÖ GPU Check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üöÄ Using device:\", device)\n",
    "\n",
    "# ‚úÖ Convert ratings to binary labels (1 = positive, 0 = negative)\n",
    "train_data['labels'] = train_data['rating'].apply(lambda x: 1 if x == 2 else 0)\n",
    "test_data['labels'] = test_data['rating'].apply(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "# ‚úÖ Reduce dataset size for quicker testing (optional for Colab runtime limits)\n",
    "train_sample = train_data.sample(10000, random_state=42).reset_index(drop=True)\n",
    "test_sample = test_data.sample(2000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# ‚úÖ Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_pandas(train_sample[['cleaned_review', 'labels']])\n",
    "test_dataset = Dataset.from_pandas(test_sample[['cleaned_review', 'labels']])\n",
    "\n",
    "# ‚úÖ Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example['cleaned_review'], padding='max_length', truncation=True)\n",
    "\n",
    "# ‚úÖ Tokenize\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# ‚úÖ Set format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# ‚úÖ Load BERT model for binary classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# ‚úÖ Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# ‚úÖ Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "# ‚úÖ Train using GPU\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ Evaluate model performance\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"üìä Evaluation results:\", eval_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KS1Ikti3v-C"
   },
   "source": [
    "After training, we evaluate BERT's performance on the test dataset by generating a detailed classification report.\n",
    "\n",
    "We:\n",
    "\n",
    "* Use the trained Trainer to predict labels on the test set\n",
    "\n",
    "* Extract true and predicted labels\n",
    "\n",
    "* Compute precision, recall, F1-score, and support using classification_report\n",
    "\n",
    "This helps us understand how well the model distinguishes between positive and negative reviews, and whether it performs consistently across both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "y_true = predictions.label_ids\n",
    "y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ckgpUdkg4Iu-"
   },
   "source": [
    "The BERT model achieved 87% accuracy with balanced precision, recall, and F1-scores (0.87) for both negative and positive reviews.\n",
    "This shows strong, consistent performance across classes with no sign of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('/content/drive/MyDrive/bert_sentiment_model')\n",
    "tokenizer.save_pretrained('/content/drive/MyDrive/bert_sentiment_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykoueUAV4V6N"
   },
   "source": [
    "We use the trained model in a HuggingFace pipeline to make predictions on new review text.\n",
    "\n",
    "This shows the model‚Äôs ability to generalize and classify unseen user feedback correctly and confidently.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedback = \"The app is easy to use and the interface is very clean. Great experience!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, pipeline\n",
    "import torch\n",
    "\n",
    "# === Load your model & tokenizer ===\n",
    "model_path = \"/content/drive/MyDrive/bert_sentiment_model\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# === Set device ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# === Create pipeline ===\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "# === Example feedback ===\n",
    "example_feedback = \"I love how fast the checkout process is. Super smooth and easy to use!\"\n",
    "\n",
    "# === Run prediction ===\n",
    "result = sentiment_pipeline(example_feedback)\n",
    "label = result[0]['label']\n",
    "score = result[0]['score']\n",
    "\n",
    "# === Interpret label ===\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"Negative\",\n",
    "    \"LABEL_1\": \"Positive\"\n",
    "}\n",
    "print(f\"üìù Feedback: {example_feedback}\")\n",
    "print(f\"‚úÖ Predicted Sentiment: {label_map[label]} (Confidence: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEnoeAYQ4Yow"
   },
   "source": [
    "We pass in a clearly negative review to confirm that the model accurately classifies it as negative, with high confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_feedback = \"This product is terrible. It broke after two days and the customer support was unhelpful.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sentiment_pipeline(example_feedback)\n",
    "label = result[0]['label']\n",
    "score = result[0]['score']\n",
    "\n",
    "label_map = {\n",
    "    \"LABEL_0\": \"Negative\",\n",
    "    \"LABEL_1\": \"Positive\"\n",
    "}\n",
    "\n",
    "print(f\"üìù Feedback: {example_feedback}\")\n",
    "print(f\"‚úÖ Predicted Sentiment: {label_map[label]} (Confidence: {score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btsDE6eB4cZv"
   },
   "source": [
    "### Classical ML Models with TF-IDF\n",
    "We train baseline models (Naive Bayes, Logistic Regression, and SVM) using TF-IDF vectorized features.\n",
    "\n",
    "This allows for comparison between classical approaches and BERT, highlighting the performance gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Sample smaller dataset for classical ML\n",
    "sample_train = train_data.sample(10000, random_state=42)\n",
    "sample_test = test_data.sample(2000, random_state=42)\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = sample_train['cleaned_review']\n",
    "y_train = sample_train['rating']\n",
    "X_test = sample_test['cleaned_review']\n",
    "y_test = sample_test['rating']\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ulxDjCU4ipI"
   },
   "source": [
    "### Naive Bayes Model\n",
    "We fit a Multinomial Naive Bayes model and generate a classification report to evaluate its performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"üîπ Naive Bayes Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_nb, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXldEH5-4oMH"
   },
   "source": [
    "### Logistic Regression (with Grid Search)\n",
    "We use GridSearchCV to tune hyperparameters and train a Logistic Regression model.\n",
    "The report shows improvements over Naive Bayes, especially in accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=1000)\n",
    "params_logreg = {'C': [0.01, 0.1, 1, 10]}\n",
    "grid_logreg = GridSearchCV(logreg, params_logreg, cv=3, scoring='accuracy')\n",
    "grid_logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred_logreg = grid_logreg.predict(X_test_tfidf)\n",
    "\n",
    "print(\"üîπ Logistic Regression Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_logreg, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RTRoJImM4sQw"
   },
   "source": [
    "### Support Vector Machine (SVM)\n",
    "We train a linear-kernel SVM classifier and evaluate it using the same test data.\n",
    "Its performance is close to Logistic Regression, with balanced precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "print(\"üîπ SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=[\"Negative\", \"Positive\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úÖ Accuracy Summary:\")\n",
    "print(f\"Naive Bayes Accuracy: {accuracy_score(y_test, y_pred_nb):.4f}\")\n",
    "print(f\"Logistic Regression Accuracy (best params): {accuracy_score(y_test, y_pred_logreg):.4f}\")\n",
    "print(f\"SVM Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA870c_cS8L4"
   },
   "source": [
    "Among the baseline models, Logistic Regression achieved the highest performance with an accuracy of 83.2%. However, the BERT-based model significantly outperformed all classical methods, reaching an accuracy of 87% with balanced precision, recall, and F1-score. This improvement demonstrates the value of using transformer-based architectures for sentiment analysis tasks, especially when dealing with nuanced textual data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
